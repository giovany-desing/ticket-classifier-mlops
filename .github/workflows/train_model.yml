name: ðŸ¤– Train ML Model with DVC

on:
  workflow_dispatch:
    inputs:
      description:
        description: 'DescripciÃ³n del entrenamiento'
        required: false
        default: 'Entrenamiento manual'
  
  push:
    branches:
      - main
    paths:
      - 'data-tickets-train/**'
      - 'scripts/train_model.py'
      - 'utils/preprocessing_data.py'
      - 'config.yaml'
      - 'requirements.txt'

env:
  PYTHON_VERSION: '3.9'
  AWS_REGION: us-east-1

jobs:
  train:
    name: ðŸš€ Train and Version Models
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      # ======================================================================
      # SETUP
      # ======================================================================
      
      - name: ðŸ“¥ Checkout repository
        uses: actions/checkout@v4
      
      - name: ðŸ Setup Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: ðŸ“¦ Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
      
      - name: ðŸ“š Download NLTK data
        run: |
          python -c "
          import nltk
          nltk.download('punkt', quiet=True)
          nltk.download('punkt_tab', quiet=True)
          nltk.download('stopwords', quiet=True)
          print('âœ“ NLTK resources ready')
          "
      
      # ======================================================================
      # DVC SETUP
      # ======================================================================
      
      - name: ðŸ—„ï¸ Pull dataset from S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ env.AWS_REGION }}
        run: |
          echo "ðŸ“¥ Downloading dataset..."
          dvc pull data-tickets-train/dataset_tickets.csv.dvc
          ls -lh data-tickets-train/
      
      # ======================================================================
      # TRAINING
      # ======================================================================
      
      - name: ðŸš€ Train models
        env:
          CI: true
          GITHUB_ACTIONS: true
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ env.AWS_REGION }}
        run: |
          echo "ðŸŽ¯ Training models..."
          python scripts/train_model.py
          echo "âœ“ Training completed"
      
      # ======================================================================
      # SAVE ARTIFACTS (PARA DESCARGA MANUAL)
      # ======================================================================
      
      - name: ðŸ’¾ Upload trained model
        uses: actions/upload-artifact@v4
        if: success()
        with:
          name: trained-model
          path: |
            models/best_model.pkl
            models/best_model.pkl.dvc
            models/best_model_metadata.json
          retention-days: 90
      
      - name: ðŸ“Š Upload MLflow logs
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: mlflow-runs
          path: mlruns/
          retention-days: 30
      
      # ======================================================================
      # SUMMARY
      # ======================================================================
      
      - name: ðŸ“‹ Generate summary
        if: always()
        run: |
          echo "# ðŸŽ¯ Training Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "models/best_model_metadata.json" ]; then
            echo "## âœ… Training Completed Successfully" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            python << 'EOF' >> $GITHUB_STEP_SUMMARY
          import json
          from pathlib import Path
          
          with open('models/best_model_metadata.json', 'r') as f:
              metadata = json.load(f)
          
          print(f"**Best Model:** {metadata['model_name']}")
          print(f"**F1-Score:** {metadata['f1_score']:.4f}")
          print(f"**Timestamp:** {metadata['timestamp']}")
          print(f"**Environment:** {metadata['environment']}")
          print("")
          
          # Verificar DVC
          dvc_file = Path('models/best_model.pkl.dvc')
          if dvc_file.exists():
              print(f"**DVC Status:** âœ… Versioned in S3")
          
          print("")
          print("### ðŸ“Š Model Comparison")
          print("")
          print("| Model | F1-Score | Accuracy | Precision | Recall |")
          print("|-------|----------|----------|-----------|--------|")
          
          results = sorted(
              metadata['all_results'].items(),
              key=lambda x: x[1]['f1_score'],
              reverse=True
          )
          
          for rank, (nombre, metricas) in enumerate(results, 1):
              emoji = "ðŸ¥‡" if rank == 1 else "ðŸ¥ˆ" if rank == 2 else "ðŸ¥‰" if rank == 3 else ""
              print(f"| {emoji} {nombre} | {metricas['f1_score']:.4f} | {metricas['accuracy']:.4f} | {metricas['precision']:.4f} | {metricas['recall']:.4f} |")
          
   